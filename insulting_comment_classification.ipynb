{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VASSILIS PANAGAKIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train and test sets\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/impermium_verification_set.csv\", usecols=['Date','Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace train set's NaN values of column 'Date'\n",
    "train['Date'].fillna('00000000000000Z', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display some samples of our initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
       "2       0  00000000000000Z  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3       0  00000000000000Z  \"listen if you dont wanna get married to a man...\n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>\"like this if you are a tribe fan\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>\"you're idiot.......................\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>\"I am a woman Babs, and the only \"war on women...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>\"WOW &amp; YOU BENEFITTED SO MANY WINS THIS YEAR F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>\"haha green me red you now loser whos winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                                            Comment\n",
       "0  20120603163526Z                 \"like this if you are a tribe fan\"\n",
       "1  20120531215447Z              \"you're idiot.......................\"\n",
       "2  20120823164228Z  \"I am a woman Babs, and the only \"war on women...\n",
       "3  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...\n",
       "4  20120602223825Z  \"haha green me red you now loser whos winning ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test set labels into a dataframe\n",
    "test_Y = pd.read_csv(\"data/impermium_verification_labels.csv\", usecols=['Insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays for sets' labels\n",
    "train_y = np.asarray(train['Insult'].tolist()) \n",
    "test_y = np.asarray(test_Y['Insult'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that removes all @mentions, links and non alphabetic strings \n",
    "def clean_content(text):\n",
    "    \n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text) #remove text with @ prefix\n",
    "    text = re.sub(r'http\\S+', '', text) #remove text with http prefix (links)  \n",
    "    text = re.sub(r'\\\\\\w+', '', str(text)) #remove text after backslash\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text) #remove text containing 2 or less characters\n",
    "    \n",
    "    text =  ''.join(ch for ch in text if ch.isalpha() or ch == ' ')\n",
    "    \n",
    "    text = text.lower() #convert text into lowercase\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column for each set containing the data of the first stage of processing\n",
    "for index, row in train.iterrows():\n",
    "    train.loc[index,'ProcessedComment'] = clean_content(train.loc[index,'Comment'])\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    test.loc[index,'ProcessedComment'] = clean_content(test.loc[index,'Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display some samples of our processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>you fuck your dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "      <td>really don understand your point  seems that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "      <td>canadians can and has been wrong before now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "      <td>listen  you dont wanna get married   man   wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "      <td>chi             giang       ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"   \n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...   \n",
       "2       0  00000000000000Z  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...   \n",
       "3       0  00000000000000Z  \"listen if you dont wanna get married to a man...   \n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...   \n",
       "\n",
       "                                    ProcessedComment  \n",
       "0                                  you fuck your dad  \n",
       "1   really don understand your point  seems that ...  \n",
       "2    canadians can and has been wrong before now ...  \n",
       "3  listen  you dont wanna get married   man   wom...  \n",
       "4                    chi             giang       ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>\"like this if you are a tribe fan\"</td>\n",
       "      <td>like this  you are  tribe fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>\"you're idiot.......................\"</td>\n",
       "      <td>you idiot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>\"I am a woman Babs, and the only \"war on women...</td>\n",
       "      <td>woman babs and the only war  women  see  co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>\"WOW &amp; YOU BENEFITTED SO MANY WINS THIS YEAR F...</td>\n",
       "      <td>wow  you benefitted  many wins this year from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>\"haha green me red you now loser whos winning ...</td>\n",
       "      <td>haha green  red you now loser whos winning now...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                                            Comment  \\\n",
       "0  20120603163526Z                 \"like this if you are a tribe fan\"   \n",
       "1  20120531215447Z              \"you're idiot.......................\"   \n",
       "2  20120823164228Z  \"I am a woman Babs, and the only \"war on women...   \n",
       "3  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...   \n",
       "4  20120602223825Z  \"haha green me red you now loser whos winning ...   \n",
       "\n",
       "                                    ProcessedComment  \n",
       "0                      like this  you are  tribe fan  \n",
       "1                                          you idiot  \n",
       "2     woman babs and the only war  women  see  co...  \n",
       "3  wow  you benefitted  many wins this year from ...  \n",
       "4  haha green  red you now loser whos winning now...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>CLASSIFICATION</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts - Bag of Words (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transorm comments to word vectors (pre-lemmatized, unigrams, stopwords not removed)\n",
    "countVectorizer = CountVectorizer(max_df=0.99, min_df=1, max_features=400, ngram_range=(1,1), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = countVectorizer.fit_transform(train['ProcessedComment'])\n",
    "\n",
    "vectors = []\n",
    "for v in bow.toarray():\n",
    "    vectors.append(v)\n",
    "\n",
    "#create a column in train set with comments' words counts   \n",
    "train['WordVecs'] = pd.Series(vectors,index=train.index)\n",
    "\n",
    "#apply transformation to test set\n",
    "bow1 = countVectorizer.transform(test['ProcessedComment'])\n",
    "\n",
    "vectors1 = []\n",
    "for v in bow1.toarray():\n",
    "    vectors1.append(v)\n",
    "\n",
    "#create a column in test set with comments' words counts   \n",
    "test['WordVecs'] = pd.Series(vectors1,index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays containing word vectors\n",
    "train_bow = np.asarray(train['WordVecs'].tolist())\n",
    "test_bow = np.asarray(test['WordVecs'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table to display the accuracy and F1-score of the Naive Bayes Classifiers \n",
    "nb_dic = {'GNB':['-','-','-'], 'MNB':['-','-','-'], 'MNB-LEM':['-','-','-'], 'MNB-SW':['-','-','-'], \n",
    "          'MNB-BG':['-','-','-'], 'MNB-LS':['-','-','-']}\n",
    "\n",
    "nb_df = pd.DataFrame.from_dict(nb_dic, orient='index', columns=['Accuracy','F1_Macro','F1_Weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal (Gaussian) Naive Bayes Classifier using word counts features without lemmatization, stopwords removal and bigrams \n",
    "gnb_bow = GaussianNB() \n",
    "gnb_bow.fit(train_bow, train_y)\n",
    "y_pred_gnb_bow = gnb_bow.predict(test_bow) #prediction on test set\n",
    "\n",
    "nb_df.loc['GNB','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_gnb_bow) * 100)\n",
    "nb_df.loc['GNB','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_gnb_bow, average='macro') * 100)\n",
    "nb_df.loc['GNB','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_gnb_bow, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier using word counts features \n",
    "#without lemmatization, stopwords removal, bigrams and Laplace Smoothing (Laplace smoothing parameter ~= 0)\n",
    "mnb_bow = MultinomialNB(alpha=1e-10) \n",
    "mnb_bow.fit(train_bow, train_y)\n",
    "y_pred_mnb_bow = mnb_bow.predict(test_bow) #prediction on test set\n",
    "\n",
    "nb_df.loc['MNB','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_mnb_bow) * 100)\n",
    "nb_df.loc['MNB','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow, average='macro') * 100)\n",
    "nb_df.loc['MNB','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts - Bag of Words (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "\n",
    "tokens = []\n",
    "for index, row in train.iterrows(): #add each comment's token in a list\n",
    "    toks = tknzr.tokenize(train.loc[index,'ProcessedComment'])\n",
    "    tokens.append(toks)\n",
    "\n",
    "#create a column in train set with comments' tokens \n",
    "train['Tokens'] = pd.Series(tokens,index=train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for index, row in test.iterrows(): #add each comment's token in a list\n",
    "    toks = tknzr.tokenize(test.loc[index,'ProcessedComment'])\n",
    "    tokens.append(toks)\n",
    "    \n",
    "#create a column in test set with comments' tokens   \n",
    "test['Tokens'] = pd.Series(tokens,index=test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization on train set\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemComment = []\n",
    "for index, row in train.iterrows(): #add each token's lemma in a list\n",
    "    lemmas = []\n",
    "    for token in train.loc[index,'Tokens']:\n",
    "        lemmas.append(lemmatizer.lemmatize(token)) #lemmatize each token\n",
    "    lemComment.append(lemmas)\n",
    "\n",
    "#replace train set's tokens with their lemmas\n",
    "train.drop(['Tokens'],1,inplace=True)\n",
    "train['Tokens'] = pd.Series(lemComment,index=train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization on test set\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemComment = []\n",
    "for index, row in test.iterrows(): #add each token's lemma in a list\n",
    "    lemmas = []\n",
    "    for token in test.loc[index,'Tokens']:\n",
    "        lemmas.append(lemmatizer.lemmatize(token)) #lemmatize each token\n",
    "    lemComment.append(lemmas)\n",
    "\n",
    "#replace test set's tokens with their lemmas    \n",
    "test.drop(['Tokens'],1,inplace=True)\n",
    "test['Tokens'] = pd.Series(lemComment,index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column in train and test sets with lemmatized comments \n",
    "train['LemComment']= train['Tokens'].str.join(' ') \n",
    "test['LemComment']= test['Tokens'].str.join(' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transorm comments to word vectors (lemmatized, unigrams, stopwords not removed)\n",
    "countVectorizer = CountVectorizer(max_df=0.99, min_df=1, max_features=400, ngram_range=(1,1), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = countVectorizer.fit_transform(train['LemComment'])\n",
    "\n",
    "vectors = []\n",
    "for v in bow.toarray():\n",
    "    vectors.append(v)\n",
    "\n",
    "#create a column in train set with lemmatized comments' words counts   \n",
    "train['LemWordVecs'] = pd.Series(vectors,index=train.index)\n",
    "\n",
    "#apply transformation to test set\n",
    "bow1 = countVectorizer.transform(test['LemComment'])\n",
    "\n",
    "vectors1 = []\n",
    "for v in bow1.toarray():\n",
    "    vectors1.append(v)\n",
    "\n",
    "#create a column in test set with lemmatized comments' words counts   \n",
    "test['LemWordVecs'] = pd.Series(vectors1,index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays containing word vectors after lemmatization\n",
    "train_bow_lem = np.asarray(train['LemWordVecs'].tolist())\n",
    "test_bow_lem = np.asarray(test['LemWordVecs'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier using word counts features after lemmatization\n",
    "#without stopwords removal, bigrams and Laplace Smoothing (Laplace smoothing parameter ~= 0)\n",
    "mnb_bow_lem = MultinomialNB(alpha=1e-10) \n",
    "mnb_bow_lem.fit(train_bow_lem, train_y)\n",
    "y_pred_mnb_bow_lem = mnb_bow_lem.predict(test_bow_lem) #prediction on test set\n",
    "\n",
    "nb_df.loc['MNB-LEM','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_mnb_bow_lem) * 100)\n",
    "nb_df.loc['MNB-LEM','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow_lem, average='macro') * 100)\n",
    "nb_df.loc['MNB-LEM','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow_lem, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transorm comments to word vectors (pre-lemmatized, unigrams, stopwords removed)\n",
    "countVectorizer = CountVectorizer(max_df=0.99, min_df=1, max_features=400, stop_words='english', \n",
    "                                      ngram_range=(1,1), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = countVectorizer.fit_transform(train['ProcessedComment'])\n",
    "\n",
    "vectors = []\n",
    "for v in bow.toarray():\n",
    "    vectors.append(v)\n",
    "\n",
    "#create a column in train set with (stopwords removed) comments' words counts   \n",
    "train['SWordVecs'] = pd.Series(vectors,index=train.index)\n",
    "\n",
    "#apply transformation to test set\n",
    "bow1 = countVectorizer.transform(test['ProcessedComment'])\n",
    "\n",
    "vectors1 = []\n",
    "for v in bow1.toarray():\n",
    "    vectors1.append(v)\n",
    "\n",
    "#create a column in test set with (stopwords removed) comments' words counts   \n",
    "test['SWordVecs'] = pd.Series(vectors1,index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays containing word vectors with removed stopwords\n",
    "train_bow_sw = np.asarray(train['SWordVecs'].tolist())\n",
    "test_bow_sw = np.asarray(test['SWordVecs'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier using word counts features with stopwords removal\n",
    "#without lemmatization, bigrams and Laplace Smoothing (Laplace smoothing parameter ~= 0)\n",
    "mnb_bow_sw = MultinomialNB(alpha=1e-10) \n",
    "mnb_bow_sw.fit(train_bow_sw, train_y)\n",
    "y_pred_mnb_bow_sw = mnb_bow_sw.predict(test_bow_sw) #prediction on test set\n",
    "\n",
    "nb_df.loc['MNB-SW','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_mnb_bow_sw) * 100)\n",
    "nb_df.loc['MNB-SW','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow_sw, average='macro') * 100)\n",
    "nb_df.loc['MNB-SW','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow_sw, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transorm comments to word vectors (pre-lemmatized, bigrams, stopwords not removed)\n",
    "countVectorizer = CountVectorizer(max_df=0.99, min_df=1, max_features=400, ngram_range=(2,2), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = countVectorizer.fit_transform(train['ProcessedComment'])\n",
    "\n",
    "vectors = []\n",
    "for v in bow.toarray():\n",
    "    vectors.append(v)\n",
    "\n",
    "#create a column in train set with (bigram) comments' words counts   \n",
    "train['BGWordVecs'] = pd.Series(vectors,index=train.index)\n",
    "\n",
    "#apply transformation to test set\n",
    "bow1 = countVectorizer.transform(test['ProcessedComment'])\n",
    "\n",
    "vectors1 = []\n",
    "for v in bow1.toarray():\n",
    "    vectors1.append(v)\n",
    "\n",
    "#create a column in test set with (bigram) comments' words counts   \n",
    "test['BGWordVecs'] = pd.Series(vectors1,index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays containing word vectors with bigrams\n",
    "train_bow_bg = np.asarray(train['BGWordVecs'].tolist())\n",
    "test_bow_bg = np.asarray(test['BGWordVecs'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier using word counts features with bigrams\n",
    "#without lemmatization, stopwords removal and Laplace Smoothing (Laplace smoothing parameter ~= 0)\n",
    "mnb_bow_bg = MultinomialNB(alpha=1e-10) \n",
    "mnb_bow_bg.fit(train_bow_bg, train_y)\n",
    "y_pred_mnb_bow_bg = mnb_bow_bg.predict(test_bow_bg) #prediction on test set\n",
    "\n",
    "nb_df.loc['MNB-BG','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_mnb_bow_bg) * 100)\n",
    "nb_df.loc['MNB-BG','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow_bg, average='macro') * 100)\n",
    "nb_df.loc['MNB-BG','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow_bg, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier using word counts features with Laplace Smoothing \n",
    "#without lemmatization, stopwords removal and bigrams\n",
    "mnb_bow2 = MultinomialNB(alpha=1.0) \n",
    "mnb_bow2.fit(train_bow, train_y)\n",
    "y_pred_mnb_bow2 = mnb_bow2.predict(test_bow) #prediction on test set\n",
    "\n",
    "nb_df.loc['MNB-LS','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_mnb_bow2) * 100)\n",
    "nb_df.loc['MNB-LS','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow2, average='macro') * 100)\n",
    "nb_df.loc['MNB-LS','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_mnb_bow2, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display some samples of our data with the new inserted word vectors columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "      <th>WordVecs</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>LemComment</th>\n",
       "      <th>LemWordVecs</th>\n",
       "      <th>SWordVecs</th>\n",
       "      <th>BGWordVecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>you fuck your dad</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[you, fuck, your, dad]</td>\n",
       "      <td>you fuck your dad</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "      <td>really don understand your point  seems that ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[really, don, understand, your, point, seems, ...</td>\n",
       "      <td>really don understand your point seems that yo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "      <td>canadians can and has been wrong before now ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[canadian, can, and, ha, been, wrong, before, ...</td>\n",
       "      <td>canadian can and ha been wrong before now and ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "      <td>listen  you dont wanna get married   man   wom...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[listen, you, dont, wanna, get, married, man, ...</td>\n",
       "      <td>listen you dont wanna get married man woman do...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "      <td>chi             giang       ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[chi, giang, chu, khi, sau, chi, tranh, con]</td>\n",
       "      <td>chi giang chu khi sau chi tranh con</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"   \n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...   \n",
       "2       0  00000000000000Z  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...   \n",
       "3       0  00000000000000Z  \"listen if you dont wanna get married to a man...   \n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...   \n",
       "\n",
       "                                    ProcessedComment  \\\n",
       "0                                  you fuck your dad   \n",
       "1   really don understand your point  seems that ...   \n",
       "2    canadians can and has been wrong before now ...   \n",
       "3  listen  you dont wanna get married   man   wom...   \n",
       "4                    chi             giang       ...   \n",
       "\n",
       "                                            WordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0                             [you, fuck, your, dad]   \n",
       "1  [really, don, understand, your, point, seems, ...   \n",
       "2  [canadian, can, and, ha, been, wrong, before, ...   \n",
       "3  [listen, you, dont, wanna, get, married, man, ...   \n",
       "4       [chi, giang, chu, khi, sau, chi, tranh, con]   \n",
       "\n",
       "                                          LemComment  \\\n",
       "0                                  you fuck your dad   \n",
       "1  really don understand your point seems that yo...   \n",
       "2  canadian can and ha been wrong before now and ...   \n",
       "3  listen you dont wanna get married man woman do...   \n",
       "4                chi giang chu khi sau chi tranh con   \n",
       "\n",
       "                                         LemWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           SWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          BGWordVecs  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "      <th>WordVecs</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>LemComment</th>\n",
       "      <th>LemWordVecs</th>\n",
       "      <th>SWordVecs</th>\n",
       "      <th>BGWordVecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>\"like this if you are a tribe fan\"</td>\n",
       "      <td>like this  you are  tribe fan</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[like, this, you, are, tribe, fan]</td>\n",
       "      <td>like this you are tribe fan</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>\"you're idiot.......................\"</td>\n",
       "      <td>you idiot</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[you, idiot]</td>\n",
       "      <td>you idiot</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>\"I am a woman Babs, and the only \"war on women...</td>\n",
       "      <td>woman babs and the only war  women  see  co...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[woman, babs, and, the, only, war, woman, see,...</td>\n",
       "      <td>woman babs and the only war woman see coming f...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>\"WOW &amp; YOU BENEFITTED SO MANY WINS THIS YEAR F...</td>\n",
       "      <td>wow  you benefitted  many wins this year from ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[wow, you, benefitted, many, win, this, year, ...</td>\n",
       "      <td>wow you benefitted many win this year from his...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>\"haha green me red you now loser whos winning ...</td>\n",
       "      <td>haha green  red you now loser whos winning now...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[haha, green, red, you, now, loser, who, winni...</td>\n",
       "      <td>haha green red you now loser who winning now m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                                            Comment  \\\n",
       "0  20120603163526Z                 \"like this if you are a tribe fan\"   \n",
       "1  20120531215447Z              \"you're idiot.......................\"   \n",
       "2  20120823164228Z  \"I am a woman Babs, and the only \"war on women...   \n",
       "3  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...   \n",
       "4  20120602223825Z  \"haha green me red you now loser whos winning ...   \n",
       "\n",
       "                                    ProcessedComment  \\\n",
       "0                      like this  you are  tribe fan   \n",
       "1                                          you idiot   \n",
       "2     woman babs and the only war  women  see  co...   \n",
       "3  wow  you benefitted  many wins this year from ...   \n",
       "4  haha green  red you now loser whos winning now...   \n",
       "\n",
       "                                            WordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0                 [like, this, you, are, tribe, fan]   \n",
       "1                                       [you, idiot]   \n",
       "2  [woman, babs, and, the, only, war, woman, see,...   \n",
       "3  [wow, you, benefitted, many, win, this, year, ...   \n",
       "4  [haha, green, red, you, now, loser, who, winni...   \n",
       "\n",
       "                                          LemComment  \\\n",
       "0                        like this you are tribe fan   \n",
       "1                                          you idiot   \n",
       "2  woman babs and the only war woman see coming f...   \n",
       "3  wow you benefitted many win this year from his...   \n",
       "4  haha green red you now loser who winning now m...   \n",
       "\n",
       "                                         LemWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           SWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          BGWordVecs  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy & F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>48.501%</td>\n",
       "      <td>41.808%</td>\n",
       "      <td>41.092%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB</th>\n",
       "      <td>62.192%</td>\n",
       "      <td>62.123%</td>\n",
       "      <td>62.064%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LEM</th>\n",
       "      <td>62.416%</td>\n",
       "      <td>62.309%</td>\n",
       "      <td>62.237%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-SW</th>\n",
       "      <td>65.861%</td>\n",
       "      <td>64.253%</td>\n",
       "      <td>64.528%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-BG</th>\n",
       "      <td>59.642%</td>\n",
       "      <td>57.750%</td>\n",
       "      <td>58.074%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LS</th>\n",
       "      <td>62.506%</td>\n",
       "      <td>62.446%</td>\n",
       "      <td>62.392%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy F1_Macro F1_Weighted\n",
       "GNB      48.501%  41.808%     41.092%\n",
       "MNB      62.192%  62.123%     62.064%\n",
       "MNB-LEM  62.416%  62.309%     62.237%\n",
       "MNB-SW   65.861%  64.253%     64.528%\n",
       "MNB-BG   59.642%  57.750%     58.074%\n",
       "MNB-LS   62.506%  62.446%     62.392%"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns a list [fractionNouns, fractionVerbs, fractionAdverbs, fractionAdjectives] for every comment\n",
    "def posFractions(df):\n",
    "    pos_tags = []\n",
    "\n",
    "    for index, row in df.iterrows(): #repeat for every row in dataframe\n",
    "        nounCount, verbCount, adverbCount, adjectiveCount = (0,0,0,0) #counter for each part of speech\n",
    "        \n",
    "        tokens = df.loc[index,'Tokens'] #get 'Tokens' column from dataframe\n",
    "        pos_tokens = pos_tag(tokens) #find what part of speech is every token\n",
    "\n",
    "        #increase counter by 1 for the correct part of speech\n",
    "        for item in pos_tokens:\n",
    "            if item[1] == 'NN' or item[1] == 'NNP':\n",
    "                nounCount+=1\n",
    "            if item[1] == 'VB' or item[1] == 'VBD' or item[1] == 'VBG' or item[1] == 'VBN':\n",
    "                verbCount+=1\n",
    "            if item[1] == 'RB' or item[1] == 'RBR' or item[1] == 'RBS':\n",
    "                adverbCount+=1\n",
    "            if item[1] == item[1] == 'JJ' or item[1] == 'JJR' or item[1] == 'JJS':\n",
    "                adjectiveCount+=1\n",
    "        \n",
    "        #create zero list if there aren't any tokens\n",
    "        if len(tokens) == 0:\n",
    "            posList = [0.0, 0.0, 0.0, 0.0]\n",
    "            pos_tags.append(posList)\n",
    "            continue\n",
    "\n",
    "        #create the fraction for every part of speech\n",
    "        fracNoun = nounCount / len(tokens)\n",
    "        fracVerb = verbCount / len(tokens)\n",
    "        fracAdverb = adverbCount / len(tokens)\n",
    "        fracAdjective = adjectiveCount / len(tokens)\n",
    "        \n",
    "        #create the part of speech final list\n",
    "        posList = [round(fracNoun,3), round(fracVerb,3), round(fracAdverb,3), round(fracAdjective,3)]\n",
    "        pos_tags.append(posList)\n",
    "        \n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column in train and test sets with comments' pos list  \n",
    "train['pos'] = posFractions(train)\n",
    "test['pos'] = posFractions(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display some samples of our data after 'pos' column insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "      <th>WordVecs</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>LemComment</th>\n",
       "      <th>LemWordVecs</th>\n",
       "      <th>SWordVecs</th>\n",
       "      <th>BGWordVecs</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>you fuck your dad</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[you, fuck, your, dad]</td>\n",
       "      <td>you fuck your dad</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.25, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "      <td>really don understand your point  seems that ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[really, don, understand, your, point, seems, ...</td>\n",
       "      <td>really don understand your point seems that yo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.231, 0.154, 0.077, 0.077]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "      <td>canadians can and has been wrong before now ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[canadian, can, and, ha, been, wrong, before, ...</td>\n",
       "      <td>canadian can and ha been wrong before now and ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.26, 0.08, 0.12, 0.06]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "      <td>listen  you dont wanna get married   man   wom...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[listen, you, dont, wanna, get, married, man, ...</td>\n",
       "      <td>listen you dont wanna get married man woman do...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.146, 0.146, 0.024, 0.171]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "      <td>chi             giang       ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[chi, giang, chu, khi, sau, chi, tranh, con]</td>\n",
       "      <td>chi giang chu khi sau chi tranh con</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.625, 0.0, 0.0, 0.125]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"   \n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...   \n",
       "2       0  00000000000000Z  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...   \n",
       "3       0  00000000000000Z  \"listen if you dont wanna get married to a man...   \n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...   \n",
       "\n",
       "                                    ProcessedComment  \\\n",
       "0                                  you fuck your dad   \n",
       "1   really don understand your point  seems that ...   \n",
       "2    canadians can and has been wrong before now ...   \n",
       "3  listen  you dont wanna get married   man   wom...   \n",
       "4                    chi             giang       ...   \n",
       "\n",
       "                                            WordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0                             [you, fuck, your, dad]   \n",
       "1  [really, don, understand, your, point, seems, ...   \n",
       "2  [canadian, can, and, ha, been, wrong, before, ...   \n",
       "3  [listen, you, dont, wanna, get, married, man, ...   \n",
       "4       [chi, giang, chu, khi, sau, chi, tranh, con]   \n",
       "\n",
       "                                          LemComment  \\\n",
       "0                                  you fuck your dad   \n",
       "1  really don understand your point seems that yo...   \n",
       "2  canadian can and ha been wrong before now and ...   \n",
       "3  listen you dont wanna get married man woman do...   \n",
       "4                chi giang chu khi sau chi tranh con   \n",
       "\n",
       "                                         LemWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           SWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          BGWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            pos  \n",
       "0         [0.25, 0.0, 0.0, 0.0]  \n",
       "1  [0.231, 0.154, 0.077, 0.077]  \n",
       "2      [0.26, 0.08, 0.12, 0.06]  \n",
       "3  [0.146, 0.146, 0.024, 0.171]  \n",
       "4      [0.625, 0.0, 0.0, 0.125]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "      <th>WordVecs</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>LemComment</th>\n",
       "      <th>LemWordVecs</th>\n",
       "      <th>SWordVecs</th>\n",
       "      <th>BGWordVecs</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>\"like this if you are a tribe fan\"</td>\n",
       "      <td>like this  you are  tribe fan</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[like, this, you, are, tribe, fan]</td>\n",
       "      <td>like this you are tribe fan</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.167, 0.0, 0.0, 0.167]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>\"you're idiot.......................\"</td>\n",
       "      <td>you idiot</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[you, idiot]</td>\n",
       "      <td>you idiot</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>\"I am a woman Babs, and the only \"war on women...</td>\n",
       "      <td>woman babs and the only war  women  see  co...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[woman, babs, and, the, only, war, woman, see,...</td>\n",
       "      <td>woman babs and the only war woman see coming f...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.292, 0.167, 0.0, 0.125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>\"WOW &amp; YOU BENEFITTED SO MANY WINS THIS YEAR F...</td>\n",
       "      <td>wow  you benefitted  many wins this year from ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[wow, you, benefitted, many, win, this, year, ...</td>\n",
       "      <td>wow you benefitted many win this year from his...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.417, 0.083, 0.0, 0.167]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>\"haha green me red you now loser whos winning ...</td>\n",
       "      <td>haha green  red you now loser whos winning now...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[haha, green, red, you, now, loser, who, winni...</td>\n",
       "      <td>haha green red you now loser who winning now m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.2, 0.1, 0.2, 0.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                                            Comment  \\\n",
       "0  20120603163526Z                 \"like this if you are a tribe fan\"   \n",
       "1  20120531215447Z              \"you're idiot.......................\"   \n",
       "2  20120823164228Z  \"I am a woman Babs, and the only \"war on women...   \n",
       "3  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...   \n",
       "4  20120602223825Z  \"haha green me red you now loser whos winning ...   \n",
       "\n",
       "                                    ProcessedComment  \\\n",
       "0                      like this  you are  tribe fan   \n",
       "1                                          you idiot   \n",
       "2     woman babs and the only war  women  see  co...   \n",
       "3  wow  you benefitted  many wins this year from ...   \n",
       "4  haha green  red you now loser whos winning now...   \n",
       "\n",
       "                                            WordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0                 [like, this, you, are, tribe, fan]   \n",
       "1                                       [you, idiot]   \n",
       "2  [woman, babs, and, the, only, war, woman, see,...   \n",
       "3  [wow, you, benefitted, many, win, this, year, ...   \n",
       "4  [haha, green, red, you, now, loser, who, winni...   \n",
       "\n",
       "                                          LemComment  \\\n",
       "0                        like this you are tribe fan   \n",
       "1                                          you idiot   \n",
       "2  woman babs and the only war woman see coming f...   \n",
       "3  wow you benefitted many win this year from his...   \n",
       "4  haha green red you now loser who winning now m...   \n",
       "\n",
       "                                         LemWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           SWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          BGWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                          pos  \n",
       "0    [0.167, 0.0, 0.0, 0.167]  \n",
       "1        [0.0, 0.0, 0.0, 0.0]  \n",
       "2  [0.292, 0.167, 0.0, 0.125]  \n",
       "3  [0.417, 0.083, 0.0, 0.167]  \n",
       "4        [0.2, 0.1, 0.2, 0.2]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays containing part of speech lists\n",
    "train_pos = np.asarray(train['pos'].tolist())\n",
    "test_pos = np.asarray(test['pos'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tfidf transformation to comments (unigrams & bigrams, lemmatized, stopwords removed)\n",
    "tfidfVectorizer = TfidfVectorizer(max_df=0.99, min_df=1, max_features=400, stop_words='english', \n",
    "                                      ngram_range=(1,2), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidfVectorizer.fit_transform(train['LemComment'])\n",
    "\n",
    "vectors = []\n",
    "for v in tfidf.toarray():\n",
    "    vectors.append(v)\n",
    "\n",
    "#create a column in train set with comments' words tf-idf counts transformation\n",
    "train['tf-idf'] = pd.Series(vectors,index=train.index)\n",
    "\n",
    "tfidf1 = tfidfVectorizer.transform(test['LemComment'])\n",
    "\n",
    "vectors1 = []\n",
    "for v in tfidf1.toarray():\n",
    "    vectors1.append(v)\n",
    "\n",
    "#create a column in test set with comments' words tf-idf counts transformation\n",
    "test['tf-idf'] = pd.Series(vectors1,index=test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display some samples of our data after 'tf-idf' column insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "      <th>WordVecs</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>LemComment</th>\n",
       "      <th>LemWordVecs</th>\n",
       "      <th>SWordVecs</th>\n",
       "      <th>BGWordVecs</th>\n",
       "      <th>pos</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>you fuck your dad</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[you, fuck, your, dad]</td>\n",
       "      <td>you fuck your dad</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.25, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "      <td>really don understand your point  seems that ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[really, don, understand, your, point, seems, ...</td>\n",
       "      <td>really don understand your point seems that yo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.231, 0.154, 0.077, 0.077]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "      <td>canadians can and has been wrong before now ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[canadian, can, and, ha, been, wrong, before, ...</td>\n",
       "      <td>canadian can and ha been wrong before now and ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.26, 0.08, 0.12, 0.06]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00000000000000Z</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "      <td>listen  you dont wanna get married   man   wom...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[listen, you, dont, wanna, get, married, man, ...</td>\n",
       "      <td>listen you dont wanna get married man woman do...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.146, 0.146, 0.024, 0.171]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "      <td>chi             giang       ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[chi, giang, chu, khi, sau, chi, tranh, con]</td>\n",
       "      <td>chi giang chu khi sau chi tranh con</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.625, 0.0, 0.0, 0.125]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"   \n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...   \n",
       "2       0  00000000000000Z  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...   \n",
       "3       0  00000000000000Z  \"listen if you dont wanna get married to a man...   \n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...   \n",
       "\n",
       "                                    ProcessedComment  \\\n",
       "0                                  you fuck your dad   \n",
       "1   really don understand your point  seems that ...   \n",
       "2    canadians can and has been wrong before now ...   \n",
       "3  listen  you dont wanna get married   man   wom...   \n",
       "4                    chi             giang       ...   \n",
       "\n",
       "                                            WordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0                             [you, fuck, your, dad]   \n",
       "1  [really, don, understand, your, point, seems, ...   \n",
       "2  [canadian, can, and, ha, been, wrong, before, ...   \n",
       "3  [listen, you, dont, wanna, get, married, man, ...   \n",
       "4       [chi, giang, chu, khi, sau, chi, tranh, con]   \n",
       "\n",
       "                                          LemComment  \\\n",
       "0                                  you fuck your dad   \n",
       "1  really don understand your point seems that yo...   \n",
       "2  canadian can and ha been wrong before now and ...   \n",
       "3  listen you dont wanna get married man woman do...   \n",
       "4                chi giang chu khi sau chi tranh con   \n",
       "\n",
       "                                         LemWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           SWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          BGWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            pos  \\\n",
       "0         [0.25, 0.0, 0.0, 0.0]   \n",
       "1  [0.231, 0.154, 0.077, 0.077]   \n",
       "2      [0.26, 0.08, 0.12, 0.06]   \n",
       "3  [0.146, 0.146, 0.024, 0.171]   \n",
       "4      [0.625, 0.0, 0.0, 0.125]   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "      <th>WordVecs</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>LemComment</th>\n",
       "      <th>LemWordVecs</th>\n",
       "      <th>SWordVecs</th>\n",
       "      <th>BGWordVecs</th>\n",
       "      <th>pos</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>\"like this if you are a tribe fan\"</td>\n",
       "      <td>like this  you are  tribe fan</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[like, this, you, are, tribe, fan]</td>\n",
       "      <td>like this you are tribe fan</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.167, 0.0, 0.0, 0.167]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>\"you're idiot.......................\"</td>\n",
       "      <td>you idiot</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[you, idiot]</td>\n",
       "      <td>you idiot</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>\"I am a woman Babs, and the only \"war on women...</td>\n",
       "      <td>woman babs and the only war  women  see  co...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[woman, babs, and, the, only, war, woman, see,...</td>\n",
       "      <td>woman babs and the only war woman see coming f...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.292, 0.167, 0.0, 0.125]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>\"WOW &amp; YOU BENEFITTED SO MANY WINS THIS YEAR F...</td>\n",
       "      <td>wow  you benefitted  many wins this year from ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[wow, you, benefitted, many, win, this, year, ...</td>\n",
       "      <td>wow you benefitted many win this year from his...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.417, 0.083, 0.0, 0.167]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>\"haha green me red you now loser whos winning ...</td>\n",
       "      <td>haha green  red you now loser whos winning now...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[haha, green, red, you, now, loser, who, winni...</td>\n",
       "      <td>haha green red you now loser who winning now m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.2, 0.1, 0.2, 0.2]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                                            Comment  \\\n",
       "0  20120603163526Z                 \"like this if you are a tribe fan\"   \n",
       "1  20120531215447Z              \"you're idiot.......................\"   \n",
       "2  20120823164228Z  \"I am a woman Babs, and the only \"war on women...   \n",
       "3  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...   \n",
       "4  20120602223825Z  \"haha green me red you now loser whos winning ...   \n",
       "\n",
       "                                    ProcessedComment  \\\n",
       "0                      like this  you are  tribe fan   \n",
       "1                                          you idiot   \n",
       "2     woman babs and the only war  women  see  co...   \n",
       "3  wow  you benefitted  many wins this year from ...   \n",
       "4  haha green  red you now loser whos winning now...   \n",
       "\n",
       "                                            WordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0                 [like, this, you, are, tribe, fan]   \n",
       "1                                       [you, idiot]   \n",
       "2  [woman, babs, and, the, only, war, woman, see,...   \n",
       "3  [wow, you, benefitted, many, win, this, year, ...   \n",
       "4  [haha, green, red, you, now, loser, who, winni...   \n",
       "\n",
       "                                          LemComment  \\\n",
       "0                        like this you are tribe fan   \n",
       "1                                          you idiot   \n",
       "2  woman babs and the only war woman see coming f...   \n",
       "3  wow you benefitted many win this year from his...   \n",
       "4  haha green red you now loser who winning now m...   \n",
       "\n",
       "                                         LemWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           SWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          BGWordVecs  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                          pos  \\\n",
       "0    [0.167, 0.0, 0.0, 0.167]   \n",
       "1        [0.0, 0.0, 0.0, 0.0]   \n",
       "2  [0.292, 0.167, 0.0, 0.125]   \n",
       "3  [0.417, 0.083, 0.0, 0.167]   \n",
       "4        [0.2, 0.1, 0.2, 0.2]   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays containing tf-idf vectors\n",
    "train_tfidf = np.asarray(train['tf-idf'].tolist())\n",
    "test_tfidf = np.asarray(test['tf-idf'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Input Features Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that unifies two arrays into one\n",
    "def unify_features(arr1, arr2):\n",
    "    uni = []\n",
    "    #unify the i-th element of arr1 with the corresponding element of arr2\n",
    "    for i,j in enumerate(arr1):\n",
    "        arr = np.append(arr1[i], arr2[i])\n",
    "        uni.append(arr)\n",
    "        \n",
    "    return np.asarray(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unify part of speech and word tfidf transformation vectors into one\n",
    "train_comb1 = unify_features(train_pos, train_tfidf)\n",
    "test_comb1 = unify_features(test_pos, test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part of Speech & Tfidf Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table to display the accuracy and F1-score of Support Vector Machines and Random Forests Classifiers\n",
    "comb_dic = {'SVM':['-','-','-'], 'RF':['-','-','-']}\n",
    "\n",
    "comb_df = pd.DataFrame.from_dict(comb_dic, orient='index', columns=['Accuracy','F1_Macro','F1_Weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines Linear Classifier using unified part of speech and word tfidf transformation features\n",
    "svc_comb1 = svm.SVC(C=100, kernel='linear', gamma=0.001)\n",
    "svc_comb1.fit(train_comb1, train_y)\n",
    "y_pred_svc_comb1 = svc_comb1.predict(test_comb1) #prediction on test set\n",
    "\n",
    "comb_df.loc['SVM','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_svc_comb1) * 100)\n",
    "comb_df.loc['SVM','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_svc_comb1, average='macro') * 100)\n",
    "comb_df.loc['SVM','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_svc_comb1, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forests Classifier using unified part of speech and word tfidf transformation features\n",
    "rf_comb1 = RandomForestClassifier(n_estimators=100, min_samples_split=8, min_samples_leaf=2)\n",
    "rf_comb1.fit(train_comb1, train_y)\n",
    "y_pred_rf_comb1 = rf_comb1.predict(test_comb1) #prediction on test set\n",
    "\n",
    "comb_df.loc['RF','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_rf_comb1) * 100)\n",
    "comb_df.loc['RF','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_rf_comb1, average='macro') * 100)\n",
    "comb_df.loc['RF','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_rf_comb1, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy & F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>66.309%</td>\n",
       "      <td>63.635%</td>\n",
       "      <td>63.993%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>65.817%</td>\n",
       "      <td>62.110%</td>\n",
       "      <td>62.540%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy F1_Macro F1_Weighted\n",
       "SVM  66.309%  63.635%     63.993%\n",
       "RF   65.817%  62.110%     62.540%"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression (Beat the Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table to display the accuracy and F1-score of Logistic Regression Classifiers\n",
    "lr_dic = {'LR(pos_tfidf)':['-','-','-'], 'LR(best_bow)':['-','-','-']}\n",
    "\n",
    "lr_df = pd.DataFrame.from_dict(lr_dic, orient='index', columns=['Accuracy','F1_Macro','F1_Weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech & Tfidf Unification (Same input vector used in SVM, RF Classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Classifier using unified part of speech and word tfidf transformation features\n",
    "lr_comb1 = LogisticRegression(C=1, class_weight='balanced', solver='lbfgs')\n",
    "\n",
    "lr_comb1.fit(train_comb1, train_y)\n",
    "y_pred_lr_comb1 = lr_comb1.predict(test_comb1) #prediction on test set\n",
    "\n",
    "lr_df.loc['LR(pos_tfidf)','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_lr_comb1) * 100)\n",
    "lr_df.loc['LR(pos_tfidf)','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_lr_comb1, average='macro') * 100)\n",
    "lr_df.loc['LR(pos_tfidf)','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_lr_comb1, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts (Best Parametres Combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transorm comments to word vectors (lemmatized, unigrams & bigrams, stopwords removed)\n",
    "countVectorizer = CountVectorizer(max_df=0.99, min_df=1, max_features=400, stop_words='english', \n",
    "                                      ngram_range=(1,2), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = countVectorizer.fit_transform(train['LemComment'])\n",
    "\n",
    "vectors = []\n",
    "for v in bow.toarray():\n",
    "    vectors.append(v)\n",
    "\n",
    "#create a column in train set with lemmatized comments' words counts   \n",
    "train['ExtWordVecs'] = pd.Series(vectors,index=train.index)\n",
    "\n",
    "#apply transformation to test set\n",
    "bow1 = countVectorizer.transform(test['LemComment'])\n",
    "\n",
    "vectors1 = []\n",
    "for v in bow1.toarray():\n",
    "    vectors1.append(v)\n",
    "\n",
    "#create a column in test set with lemmatized comments' words counts   \n",
    "test['ExtWordVecs'] = pd.Series(vectors1,index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays containing word counts vectors\n",
    "train_bow_ext = np.asarray(train['ExtWordVecs'].tolist())\n",
    "test_bow_ext = np.asarray(test['ExtWordVecs'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Classifier using word counts features\n",
    "lr_bow_ext = LogisticRegression(C=1, class_weight='balanced', solver='lbfgs')\n",
    "\n",
    "lr_bow_ext.fit(train_bow_ext, train_y)\n",
    "y_pred_lr_bow_ext = lr_bow_ext.predict(test_bow_ext) #prediction on test set\n",
    "\n",
    "lr_df.loc['LR(best_bow)','Accuracy'] = \"%.3f%%\" % (metrics.accuracy_score(test_y, y_pred_lr_bow_ext) * 100)\n",
    "lr_df.loc['LR(best_bow)','F1_Macro'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_lr_bow_ext, average='macro') * 100)\n",
    "lr_df.loc['LR(best_bow)','F1_Weighted'] = \"%.3f%%\" % (metrics.f1_score(test_y, y_pred_lr_bow_ext, average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy & F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR(pos_tfidf)</th>\n",
       "      <td>67.069%</td>\n",
       "      <td>66.852%</td>\n",
       "      <td>66.949%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR(best_bow)</th>\n",
       "      <td>68.635%</td>\n",
       "      <td>68.044%</td>\n",
       "      <td>68.202%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Accuracy F1_Macro F1_Weighted\n",
       "LR(pos_tfidf)  67.069%  66.852%     66.949%\n",
       "LR(best_bow)   68.635%  68.044%     68.202%"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers Final Accuracy & F-Measure Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename dataframe indecies to distinct the Classifiers in the final dataframe\n",
    "new_nb_df = nb_df.rename(index={'GNB': 'GNB(bow)', 'MNB': 'MNB(bow)', 'MNB-LEM': 'MNB-LEM(bow)', 'MNB-SW': 'MNB-SW(bow)',\n",
    "                                   'MNB-BG': 'MNB-BG(bow)', 'MNB-LS': 'MNB-LS(bow)'})\n",
    "new_comb_df = comb_df.rename(index={'SVM': 'SVM(pos_tfidf)', 'RF': 'RF(pos_tfidf)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GNB(bow)</th>\n",
       "      <td>48.501%</td>\n",
       "      <td>41.808%</td>\n",
       "      <td>41.092%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB(bow)</th>\n",
       "      <td>62.192%</td>\n",
       "      <td>62.123%</td>\n",
       "      <td>62.064%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LEM(bow)</th>\n",
       "      <td>62.416%</td>\n",
       "      <td>62.309%</td>\n",
       "      <td>62.237%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-SW(bow)</th>\n",
       "      <td>65.861%</td>\n",
       "      <td>64.253%</td>\n",
       "      <td>64.528%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-BG(bow)</th>\n",
       "      <td>59.642%</td>\n",
       "      <td>57.750%</td>\n",
       "      <td>58.074%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LS(bow)</th>\n",
       "      <td>62.506%</td>\n",
       "      <td>62.446%</td>\n",
       "      <td>62.392%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM(pos_tfidf)</th>\n",
       "      <td>66.309%</td>\n",
       "      <td>63.635%</td>\n",
       "      <td>63.993%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF(pos_tfidf)</th>\n",
       "      <td>65.817%</td>\n",
       "      <td>62.110%</td>\n",
       "      <td>62.540%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR(pos_tfidf)</th>\n",
       "      <td>67.069%</td>\n",
       "      <td>66.852%</td>\n",
       "      <td>66.949%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR(best_bow)</th>\n",
       "      <td>68.635%</td>\n",
       "      <td>68.044%</td>\n",
       "      <td>68.202%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy F1_Macro F1_Weighted\n",
       "GNB(bow)        48.501%  41.808%     41.092%\n",
       "MNB(bow)        62.192%  62.123%     62.064%\n",
       "MNB-LEM(bow)    62.416%  62.309%     62.237%\n",
       "MNB-SW(bow)     65.861%  64.253%     64.528%\n",
       "MNB-BG(bow)     59.642%  57.750%     58.074%\n",
       "MNB-LS(bow)     62.506%  62.446%     62.392%\n",
       "SVM(pos_tfidf)  66.309%  63.635%     63.993%\n",
       "RF(pos_tfidf)   65.817%  62.110%     62.540%\n",
       "LR(pos_tfidf)   67.069%  66.852%     66.949%\n",
       "LR(best_bow)    68.635%  68.044%     68.202%"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all dataframes' rows to create the final dataframe\n",
    "cl_df = pd.concat([new_nb_df, new_comb_df, lr_df])\n",
    "\n",
    "cl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted Classifiers from Best to Worst based on Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR(best_bow)</th>\n",
       "      <td>68.635%</td>\n",
       "      <td>68.044%</td>\n",
       "      <td>68.202%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR(pos_tfidf)</th>\n",
       "      <td>67.069%</td>\n",
       "      <td>66.852%</td>\n",
       "      <td>66.949%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM(pos_tfidf)</th>\n",
       "      <td>66.309%</td>\n",
       "      <td>63.635%</td>\n",
       "      <td>63.993%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-SW(bow)</th>\n",
       "      <td>65.861%</td>\n",
       "      <td>64.253%</td>\n",
       "      <td>64.528%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF(pos_tfidf)</th>\n",
       "      <td>65.817%</td>\n",
       "      <td>62.110%</td>\n",
       "      <td>62.540%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LS(bow)</th>\n",
       "      <td>62.506%</td>\n",
       "      <td>62.446%</td>\n",
       "      <td>62.392%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LEM(bow)</th>\n",
       "      <td>62.416%</td>\n",
       "      <td>62.309%</td>\n",
       "      <td>62.237%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB(bow)</th>\n",
       "      <td>62.192%</td>\n",
       "      <td>62.123%</td>\n",
       "      <td>62.064%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-BG(bow)</th>\n",
       "      <td>59.642%</td>\n",
       "      <td>57.750%</td>\n",
       "      <td>58.074%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNB(bow)</th>\n",
       "      <td>48.501%</td>\n",
       "      <td>41.808%</td>\n",
       "      <td>41.092%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy F1_Macro F1_Weighted\n",
       "LR(best_bow)    68.635%  68.044%     68.202%\n",
       "LR(pos_tfidf)   67.069%  66.852%     66.949%\n",
       "SVM(pos_tfidf)  66.309%  63.635%     63.993%\n",
       "MNB-SW(bow)     65.861%  64.253%     64.528%\n",
       "RF(pos_tfidf)   65.817%  62.110%     62.540%\n",
       "MNB-LS(bow)     62.506%  62.446%     62.392%\n",
       "MNB-LEM(bow)    62.416%  62.309%     62.237%\n",
       "MNB(bow)        62.192%  62.123%     62.064%\n",
       "MNB-BG(bow)     59.642%  57.750%     58.074%\n",
       "GNB(bow)        48.501%  41.808%     41.092%"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_cl_df = cl_df.sort_values(by=['Accuracy'], ascending=False)\n",
    "\n",
    "sort_cl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Classification Accuracy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GNB(bow)</th>\n",
       "      <td>48.501%</td>\n",
       "      <td>41.808%</td>\n",
       "      <td>41.092%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Accuracy F1_Macro F1_Weighted\n",
       "GNB(bow)  48.501%  41.808%     41.092%"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_cl_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNB-SW(bow)</th>\n",
       "      <td>65.861%</td>\n",
       "      <td>64.253%</td>\n",
       "      <td>64.528%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LS(bow)</th>\n",
       "      <td>62.506%</td>\n",
       "      <td>62.446%</td>\n",
       "      <td>62.392%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-LEM(bow)</th>\n",
       "      <td>62.416%</td>\n",
       "      <td>62.309%</td>\n",
       "      <td>62.237%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB(bow)</th>\n",
       "      <td>62.192%</td>\n",
       "      <td>62.123%</td>\n",
       "      <td>62.064%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB-BG(bow)</th>\n",
       "      <td>59.642%</td>\n",
       "      <td>57.750%</td>\n",
       "      <td>58.074%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy F1_Macro F1_Weighted\n",
       "MNB-SW(bow)   65.861%  64.253%     64.528%\n",
       "MNB-LS(bow)   62.506%  62.446%     62.392%\n",
       "MNB-LEM(bow)  62.416%  62.309%     62.237%\n",
       "MNB(bow)      62.192%  62.123%     62.064%\n",
       "MNB-BG(bow)   59.642%  57.750%     58.074%"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_cl_df[sort_cl_df.index.str.startswith('M')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *The worst Classifier based on both classification accuracy and f1 score metrics on the test set is the first one we implemented, namely the Gaussian Naive Bayes Classifier, using word counts as features. That's expected, due to the deficient pre-processing on the data. Specifically, when GNB was executed we hadn't lemmatized our data or removed the stopwords, yet. <br> Moreover, as it turned out {GNB(bow)} isn't the most efficient, even, among the Naive Bayes Classifiers, as the Multinomial Naive Bayes Classifier {MNB(bow)} turned out to be the one, even before trying the additional cleansing on our data. <br> Concering the Multinomial Naive Bayes Classifiers we can distinct a 3% constant difference of the classification accuracy rate between the best MNB Classifier {MNB-SW(bow)} (the one where stopwords are removed) and the 3 intermediate MNB Classifiers, as well as between the intermediate MNB Classifiers and the worst MNB Classifier {MNB-BG(bow)} (the one where we use bigrams instead of unigrams). Same goes to both weighted and unweighted f1 scores metrics, with even bigger rate difference between the intermediate and the worst MNB Classifiers. <br> Having said that, we can verify the notion that removing the stopwords is a necessary strategy in every dataset cleansing and specially in our case it seems to lead to better classification results than lemmatization or data smoothing. <br> Finally, regarding the rest MNB Classifiers, it's worth mentioning that the lemmatization and Laplace Smoothing techniques had almost zero effect on the classification accuracy and f1 score metrics compared with our initial {MNB(bow)} Classifier.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR(best_bow)</th>\n",
       "      <td>68.635%</td>\n",
       "      <td>68.044%</td>\n",
       "      <td>68.202%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR(pos_tfidf)</th>\n",
       "      <td>67.069%</td>\n",
       "      <td>66.852%</td>\n",
       "      <td>66.949%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Accuracy F1_Macro F1_Weighted\n",
       "LR(best_bow)   68.635%  68.044%     68.202%\n",
       "LR(pos_tfidf)  67.069%  66.852%     66.949%"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_cl_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Classification Accuracy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR(best_bow)</th>\n",
       "      <td>68.635%</td>\n",
       "      <td>68.044%</td>\n",
       "      <td>68.202%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy F1_Macro F1_Weighted\n",
       "LR(best_bow)  68.635%  68.044%     68.202%"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_cl_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *The best Classifier based on both classification accuracy and f1 score metrics on the test set is the <br> Logistic Regression Classifier {LR(best_bow)}, using words counts as features. We produced the input vector by combining the best possible parametres (unigrams & bigrams, stopwords removal) of CountVectorizer and we used it to transform our (lemmatized) dataset. Logistic Regression Classifier is, generally, one of the most effective binary classifiers. That's expected, because Logistic Regression analysis' dependent variable is exclusively binary. Furthermore, the low correlations among the predictors is an important factor of the method's effectiveness. So it turned out in our classification problem, since the {LR(best_bow)} Classifier predicted the corrent label with the highest accuracy rate, close to 70%. <br> The only paradox we could point out concerning our two LR Classifiers is that {LR(pos_tfidf)}, which uses words tfidf transformation as part of his input feature vector, can't overcome the {LR(best_bow)} Classifier's classification accuracy, which, uses word counts as features. <br> The importance of this observation, results from the fact that most of the time the tf-idf transformation method produces better classification results than the word counts method. <br> Yet, it seems that doesn't apply in the Logistic Regression Classifiers of our paradigm and same goes to the f1 score metrics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on SVM & RF Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM(pos_tfidf)</th>\n",
       "      <td>66.309%</td>\n",
       "      <td>63.635%</td>\n",
       "      <td>63.993%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF(pos_tfidf)</th>\n",
       "      <td>65.817%</td>\n",
       "      <td>62.110%</td>\n",
       "      <td>62.540%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy F1_Macro F1_Weighted\n",
       "SVM(pos_tfidf)  66.309%  63.635%     63.993%\n",
       "RF(pos_tfidf)   65.817%  62.110%     62.540%"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_comb_df.sort_values(by=['Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Finally, it's important to appose our observations about the classification results of our Support Vector Machines and Random Forests Classifiers using the unification of part of speech fractions and words tf-idf transformation as features. <br> It's obvious that the compound input feature vector (pos-tfidf) has almost the same effect on both type of Classifiers as regards to the classification accuracy. It's clear that both Classifiers produce very satisfying classification results, that place them 3rd and 4th best, relatively, in the classification accuracy standings by a small rate difference compared to the results of the two LR Classifiers. In fact, we note that, even if the Random Forests Classifier is intrinsically suited for multiclass problems it produces good results for our two-class problem. <br> The above observations apply in the exact same way to the f1 score rates of our discussed Classifiers.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
